# =================================================================
# DISTRIBUTED SCHEDULER MICROSERVICE CONFIGURATION
# =================================================================
# This configuration file supports environment variable and system property overrides
# Example: -Dscheduler.server.port=8081 or export SCHEDULER_SERVER_PORT=8081

# =================================================================
# SERVER CONFIGURATION
# =================================================================
# Server port - can be overridden with -Dscheduler.server.port=XXXX
scheduler.server.port=8080

# Unique instance identifier - auto-generated if not specified
scheduler.instance.id=scheduler-instance-${random.uuid}

# REST API context path - all endpoints will be available under this path
scheduler.context.path=/sch

# =================================================================
# KAFKA CONFIGURATION
# =================================================================
# Kafka Bootstrap Servers - supports multiple brokers separated by commas
# Examples:
#   Local: localhost:9092
#   Remote: kafka1.example.com:9092,kafka2.example.com:9092
#   AWS MSK: b-1.cluster.kafka.us-east-1.amazonaws.com:9092
#   Confluent Cloud: pkc-xxxxx.us-west-2.aws.confluent.cloud:9092
kafka.bootstrap.servers=10.32.208.35:9099

# Advanced Kafka Configuration (uncomment and customize as needed)
# kafka.producer.acks=all
# kafka.producer.retries=3
# kafka.producer.batch.size=16384
# kafka.producer.linger.ms=1
# kafka.producer.buffer.memory=33554432
# kafka.producer.compression.type=none
# kafka.consumer.auto.offset.reset=earliest
# kafka.consumer.enable.auto.commit=true
# kafka.consumer.auto.commit.interval.ms=1000

# =================================================================
# LOGGING CONFIGURATION
# =================================================================
# Root logging level - controls overall application verbosity
logging.level.root=INFO

# Scheduler package logging - set to DEBUG for detailed operation logs
logging.level.com.scheduler=DEBUG

# Third-party library logging levels
logging.level.org.apache.kafka=WARN
logging.level.org.eclipse.jetty=INFO
logging.level.org.quartz=INFO

# =================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATIONS
# =================================================================
# The following configurations show examples for different environments
# Uncomment and modify based on your deployment environment

# DEVELOPMENT ENVIRONMENT
# scheduler.server.port=8080
# kafka.bootstrap.servers=localhost:9092
# logging.level.com.scheduler=DEBUG

# PRODUCTION ENVIRONMENT
# scheduler.server.port=8080
# kafka.bootstrap.servers=kafka-cluster.prod.com:9092
# logging.level.root=WARN
# logging.level.com.scheduler=INFO

# DOCKER ENVIRONMENT
# scheduler.server.port=8080
# kafka.bootstrap.servers=kafka:9092
# scheduler.instance.id=${HOSTNAME}

# KUBERNETES ENVIRONMENT
# scheduler.server.port=8080
# kafka.bootstrap.servers=${KAFKA_SERVICE_HOST}:${KAFKA_SERVICE_PORT}
# scheduler.instance.id=${POD_NAME}

# AWS ENVIRONMENT
# scheduler.server.port=8080
# kafka.bootstrap.servers=${AWS_MSK_BOOTSTRAP_SERVERS}
# scheduler.instance.id=${AWS_INSTANCE_ID}

# =================================================================
# CLOUD KAFKA SERVICES EXAMPLES
# =================================================================

# Amazon MSK (Managed Streaming for Kafka)
# kafka.bootstrap.servers=b-1.mycluster.kafka.us-east-1.amazonaws.com:9092,b-2.mycluster.kafka.us-east-1.amazonaws.com:9092

# Confluent Cloud
# kafka.bootstrap.servers=pkc-xxxxx.us-west-2.aws.confluent.cloud:9092

# Azure Event Hubs (Kafka-compatible)
# kafka.bootstrap.servers=mynamespace.servicebus.windows.net:9093

# Google Cloud Pub/Sub (via Kafka adapter)
# kafka.bootstrap.servers=kafka.googleapis.com:9092

# =================================================================
# PERFORMANCE TUNING
# =================================================================
# Uncomment and adjust these settings for high-performance scenarios

# JVM Heap Settings (set via command line)
# -Xms1g -Xmx4g

# Kafka Producer Tuning
# kafka.producer.batch.size=32768
# kafka.producer.linger.ms=5
# kafka.producer.buffer.memory=67108864
# kafka.producer.compression.type=snappy

# Kafka Consumer Tuning  
# kafka.consumer.fetch.min.bytes=1024
# kafka.consumer.fetch.max.wait.ms=500
# kafka.consumer.max.poll.records=1000

# =================================================================
# SECURITY CONFIGURATION (Examples)
# =================================================================
# Uncomment and configure based on your security requirements

# SSL Configuration
# kafka.security.protocol=SSL
# kafka.ssl.truststore.location=/path/to/truststore.jks
# kafka.ssl.truststore.password=truststore-password
# kafka.ssl.keystore.location=/path/to/keystore.jks  
# kafka.ssl.keystore.password=keystore-password

# SASL Configuration
# kafka.security.protocol=SASL_SSL
# kafka.sasl.mechanism=PLAIN
# kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="user" password="password";

# =================================================================
# MONITORING AND HEALTH CHECKS
# =================================================================
# Add these properties for production monitoring

# Application name for logging and monitoring
application.name=distributed-scheduler
application.version=1.0.0

# Enable/disable health check endpoints (if implemented)
# management.health.enabled=true
# management.endpoints.web.exposure.include=health,info,metrics